[TOC]

# Lecture 8: Noise and Error
<br/>

## Noise and Probabilistic Target
&emsp;&emsp; can replace $f(x)$ by $P(y|x)$
<br/>

### Noise
&emsp;&emsp; noise in y, i.e. good customer, ‘mislabeled’ as bad.
&emsp;&emsp; noise in y: i.e. same customers, different labels.
&emsp;&emsp; noise in x: i.e. inaccurate customer information.




## Error Measure
&emsp;&emsp; affect ‘ideal’ target
<br/>

## Algorithmic Error Measure
&emsp;&emsp; user-dependent $=>$ plausible or friendly
<br/>

## Weighted Classification
&emsp;&emsp; easily done by virtual ‘example copying’
<br/>

## Summary
本篇讲义主要讲了模型复杂度，数据数量级以及VC Bound的宽松程度。

在有限的$d_{\mathrm{vc}}$，足够大的$N$，足够小的$E_{in}$，我们真正能学到模型。

<br/>

### 讲义总结

**Noise and Probabilistic Target**
&emsp;&emsp; can replace $f(x)$ by $P(y|x)$
<br/>



**Error Measure**
&emsp;&emsp; affect ‘ideal’ target
<br/>

**Algorithmic Error Measure**
&emsp;&emsp; user-dependent $=>$ plausible or friendly
<br/>

**Weighted Classification**
&emsp;&emsp; easily done by virtual ‘example copying’
<br/>

### 参考文献
<a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound18fall/">《Machine Learning Foundations》(机器学习基石)—— Hsuan-Tien Lin (林轩田)</a>